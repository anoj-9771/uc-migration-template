parameters:
- name: dependencies
  type: object
  default: ['build']
- name: stage_name
  type: string
- name: variable_template_name
  type: string
- name: run_condition
  type: string

stages:
# ---------------------------------------------------------------------------
# Release to environment.
# Only execute if PR into Develop
- stage: ${{ parameters.stage_name }}
  dependsOn: ${{ parameters.dependencies }}
  condition: ${{ parameters.run_condition }}
  variables:
  - template: ../Variables/${{ parameters.variable_template_name }}.yml
  - name: kv_base_url
    value: 'https://${{ Variables.azurekeyvaultname }}.vault.azure.net/'
  - name: datalake_base_url
    value: 'https://${{ Variables.datalakestorageaccountnames }}.dfs.core.windows.net/'
  - name: blob_base_url
    value: 'https://${{ Variables.blobstorageaccountnames }}.blob.core.windows.net/'

  jobs:
  - job:
    steps:
    
    - task: DownloadBuildArtifacts@0
      inputs:
        artifactName: 'adf_arm_templates' 

    - task: AzureResourceGroupDeployment@2
      inputs:
        azureSubscription: ${{ variables.azdoresourceconnection }}
        action: 'Create Or Update Resource Group'
        resourceGroupName: ${{ variables.resourcegroupname }}
        location: $(LOCATION)
        templateLocation: 'Linked artifact'
        csmFile: '$(Pipeline.Workspace)/adf_arm_templates/ARMTemplateForFactory.json'
        overrideParameters: '-factoryName $(DATAFACTORYNAME) -LS-AzureKeyVault_properties_typeProperties_baseUrl ${{ variables.kv_base_url }} -LS_AzureDatalake_properties_typeProperties_url ${{ variables.datalake_base_url }} -LS_AzureBlobStorage_properties_typeProperties_serviceEndpoint ${{ variables.blob_base_url }} -LS-AzureDatabricks_DefinedCluster_properties_typeProperties_domain $(databricks_base_url)' 
        deploymentMode: 'Incremental'
      displayName: 'Deploy Datafactory with Access Policies'

    - task: AzureCLI@2
      displayName: 'Add Data Factory SP to AKV'
      name: add_adf_sp_to_akv
      inputs:
        azureSubscription: ${{ variables.azdoresourceconnection }}
        scriptType: bash
        scriptLocation: inlineScript
        inlineScript: |
          # adf_sp_oid=$(az ad sp list --display-name $(DATAFACTORYNAME) --output tsv --query "[].{id:objectId}") 
          adf_sp_oid=$(az resource show -g ${{ variables.resourcegroupname }} --resource-type Microsoft.DataFactory/factories -n $(DATAFACTORYNAME) --query identity.principalId --output tsv)                  
          echo "$adf_sp_oid"
          az keyvault set-policy --name "$(AZUREKEYVAULTNAME)" --secret-permissions list get --object-id "$adf_sp_oid" --query "{Status:properties.provisioningState}" --output table

    # Add Data Factory MI to Key Vault Access Policies
    - template: task-add-mi-to-akv-access-policy.yml
      parameters:
        service_connection: ${{ variables.azdoresourceconnection }}
        resource_group: ${{ variables.resourcegroupname }}
        src_resource_name: $(DATAFACTORYNAME)
        key_vault_name: $(AZUREKEYVAULTNAME)
        secret_permissions: 'list get'

    # Add Data Factory MI to SQLServer Rbac
    - template: task-add-mi-to-resource-rbac.yml
      parameters:
        service_connection: ${{ variables.azdoresourceconnection }}
        resource_group: ${{ variables.resourcegroupname }}
        src_resource_name: $(DATAFACTORYNAME)
        trg_resource_name: $(AZURESQLSERVERNAME)
        role: 'Contributor'   

    # Add Data Factory MI to DataLake Rbac
    - template: task-add-mi-to-resource-rbac.yml
      parameters:
        service_connection: ${{ variables.azdoresourceconnection }}
        resource_group: ${{ variables.resourcegroupname }}
        src_resource_name: $(DATAFACTORYNAME)
        trg_resource_name: $(datalakestorageaccountnames)
        role: 'Storage Blob Data Contributor'

    # Add Data Factory MI to Blob Storage
    - template: task-add-mi-to-resource-rbac.yml
      parameters:
        service_connection: ${{ variables.azdoresourceconnection }}
        resource_group: ${{ variables.resourcegroupname }}
        src_resource_name: $(DATAFACTORYNAME)
        trg_resource_name: $(blobstorageaccountnames)
        role: 'Storage Blob Data Contributor'   

    # Add Data Factory MI to Databricks
    - template: task-add-mi-to-resource-rbac.yml
      parameters:
        service_connection: ${{ variables.azdoresourceconnection }}
        resource_group: ${{ variables.resourcegroupname }}
        src_resource_name: $(DATAFACTORYNAME)
        trg_resource_name: $(databricksworkspacename)
        role: 'Contributor'              