# ---------------------------------------------------------------------------
# This template is called by databricks-deploy-pipeline-cicd.yml
# for each deployment stage.

parameters:
- name: dependencies
  type: object
  default: ['build']
- name: stage_name
  type: string
- name: variable_template_name
  type: string
- name: run_condition
  type: string

stages:
# ---------------------------------------------------------------------------
# Release to environment
# Only execute if PR into Develop
- stage: ${{ parameters.stage_name }}
  dependsOn: ${{ parameters.dependencies }}
  condition: ${{ parameters.run_condition }}
  variables:
  - template: ../Variables/${{ parameters.variable_template_name }}.yml
  jobs:
  # ---------------------------------------------------------------------------
  # Attach Databricks artifact and assign reference "databricks_notebooks"
  - job: 'integrate_databricks'

    steps:
    - task: DownloadBuildArtifacts@0
      inputs:
        artifactName: 'databricks_notebooks' 

    # Install latest version of Python. If specified, this must match the version on the Databricks cluster 
    - task: UsePythonVersion@0
      inputs:
        versionSpec: '3.x'

    # Install required Python modules eg. databricks-connect
    - script: |
        pip install --upgrade pip setuptools wheel databricks-cli
      displayName: 'Install required Python modules'

    # Import Key Vault secret for databricks token
    - task: AzureKeyVault@1
      displayName: 'Get Secrets'
      inputs:
        azureSubscription: ${{ variables.azdoresourceconnection }}
        keyVaultName: ${{ variables.azurekeyvaultname }}
        secretsFilter: 'databricks-token'
        
    # Authenticate with Databricks CLI
    - bash: |
        databricks configure --token <<EOF
        https://${{ variables.location }}.azuredatabricks.net
        $(DATABRICKS-TOKEN)
        EOF
      displayName: 'Authenticate with Databricks CLI'

    - task: PythonScript@0
      inputs:
        scriptSource: inline
        script: |
          def apply_substitutions(filepath, substitutions):
            print("Filepath : " + filepath)
            file = open(filepath, "rt")
            filecontent = file.read()
            for entry in substitutions:
              filecontent = filecontent.replace(entry[0], entry[1])
            file.close()
            print('--------- Content After Substitution ')
            print(filecontent)
            file = open(filepath, "wt")
            file.write(filecontent)
            file.close()

          globalConfigFile = "$(Pipeline.Workspace)/databricks_notebooks/includes/global-variables-config.py"
          globalConfigSubs = [('<ADS_ENVIRONMENT>', '$(environment)')]
          apply_substitutions(globalConfigFile, globalConfigSubs)

          
          globalVarsPython = "$(Pipeline.Workspace)/databricks_notebooks/includes/global-variables-python.py"
          globalVarsPythonSubs = [('<EDW>', '$(azureedwdbname)')]
          apply_substitutions(globalVarsPython, globalVarsPythonSubs)

          globalVarsScala = "$(Pipeline.Workspace)/databricks_notebooks/includes/global-variables-scala.scala"
          globalVarsScalaSubs = [('<EDW>', '$(azureedwdbname)')]
          apply_substitutions(globalVarsScala, globalVarsScalaSubs)

    # Remove Current Notebooks - continue on error is required for first run as there will be no build folders
    - script: |
        databricks workspace mkdirs /build;
        databricks workspace rm -r /build;
      displayName: 'Remove Current Notebooks'
      continueOnError: true

    # Update Notebooks to Databricks workspace
    - script: |
        databricks workspace mkdirs /build;
        databricks workspace import_dir -o -e $(Pipeline.Workspace)/databricks_notebooks "/build"
        echo
      displayName: 'Add updated notbooks to workspace'