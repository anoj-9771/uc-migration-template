# ---------------------------------------------------------------------------
# This template is called by azure-platform-deploy-pipeline-cicd.yml
# for each deployment stage.
#
# Pull in and out the different templates that deploy the ARM resources
# needed for your data platform solution. If your solution needs
# a service that is not yet included, build the required ARM template 
# (arm-[service-name-description].json) and step template 
# (step-deploy-arm-[service-name-description].yml). Reference the step template
# below.

parameters:
- name: stage_name
  type: string
- name: variable_template_name
  type: string
- name: run_condition
  type: string

stages:
# ---------------------------------------------------------------------------
# Release to environment
# Only execute if PR into Develop
- stage: ${{ parameters.stage_name }}
  condition: ${{ parameters.run_condition }}
  variables:
  - template: ../Variables/${{ parameters.variable_template_name }}.yml
  jobs:
  # ---------------------------------------------------------------------------
  # Attach Databricks artifact and assign reference "databricks_notebooks"
  - job: 'integrate_databricks'
    # steps:
    # - download: current
    #   artifact: databricks_notebooks
    # # Integrate Databricks
    # - template: job-integrate-databricks.yml
    #   parameters:
    #     AzDoServiceConnection: ${{ variables.azdoresourceconnection }}
    #     resource_group: '${{ variables.prefix }}-${{ variables.environment }}'
    steps:
    - download: current
      artifact: databricks_notebooks

    # Install latest version of Python. If specified, this must match the version on the Databricks cluster 
    - task: UsePythonVersion@0
      inputs:
        versionSpec: '3.x'

    # Install required Python modules eg. databricks-connect
    - script: |
      pip install --upgrade pip setuptools wheel databricks-cli
    displayName: 'Install required Python modules'
    # # Import Key Vault secret for databricks token
    # - task: AzureKeyVault@1
    #   displayName: 'Get Secrets'
    #   inputs:
    #     azureSubscription: ${{ variables.azdoresourceconnection }}
    #     keyVaultName: ${{ variables.azurekeyvaultname }}
    #     secretsFilter: 'databricks-token'
        
    # # Authenticate with Databricks CLI
    # - script: |
    #   databricks configure --token <<EOF
    #   https://${{ location }}.azuredatabricks.net
    #   $(DATABRICKS-TOKEN)
    #   EOF

    # # Remove Current Notebooks
    # - script: |
    #   databricks workspace mkdirs /build/Util;
    #   databricks workspace mkdirs /build/DataProcessing;
    #   databricks workspace rm -r /build/Util;
    #   databricks workspace rm -r /build/DataProcessing;

    # # Update Notebooks to Databricks workspace
    # - script: |
    #   databricks workspace mkdirs /build/Util;
    #   databricks workspace mkdirs /build/DataProcessing;
    #   for filename in _Databricks/notebook/*-dp.py; 
    #   do
    #       databricks workspace import --language PYTHON --format SOURCE --overwrite "${filename}" "/build/DataProcessing/${filename##*/}"
    #   done;
    #   for filename in _Databricks/notebook/*-dp.scala; 
    #   do
    #       databricks workspace import --language SCALA --format SOURCE --overwrite "${filename}" "/build/DataProcessing/${filename##*/}"
    #   done;
    #   for filename in _Databricks/notebook/*-ut.py; 
    #   do
    #       databricks workspace import --language PYTHON --format SOURCE --overwrite "${filename}" "/build/Util/${filename##*/}"
    #   done;
    #   for filename in _Databricks/notebook/*-ut.scala; 
    #   do
    #       databricks workspace import --language SCALA --format SOURCE --overwrite "${filename}" "/build/Util/${filename##*/}"
    #   done;
    #   echo
