# parameters:
# - name: AzDoServiceConnection # name of the parameter; required
#   type: string # data type of the parameter; required

# # jobs:
# # - job: 'integrate_databricks'
#   # dependsOn: deploy_arm

# # Specify the OS for agent

# # Import the batabricks_notbook artifact
# steps:
# - download: current
#   artifact: databricks_notebooks

# # Install latest version of Python. If specified, this must match the version on the Databricks cluster 
# - task: UsePythonVersion@0
#   inputs:
#     versionSpec: '3.x'

# # Install required Python modules eg. databricks-connect.
# - script: |
#   pip install --upgrade pip setuptools wheel databricks-cli

# # Import Key Vault secret for databricks token
# - task: AzureKeyVault@1
#   displayName: 'Get Secrets'
#   inputs:
#     azureSubscription: ${{ parameters.AzDoServiceConnection }}
#     keyVaultName: $(AZUREKEYVAULTNAME)
#     secretsFilter: 'databricks-token'
    
# # Authenticate with Databricks CLI
# - script: |
#   databricks configure --token <<EOF
#   https://${{ location }}.azuredatabricks.net
#   $(DATABRICKS-TOKEN)
#   EOF

# # Remove Current Notebooks
# - script: |
#   databricks workspace mkdirs /build;
#   databricks workspace rm -r /build;

# # Update Notebooks to Databricks workspace.
# - script: |
#   databricks workspace mkdirs /build/;
#   for filename in _Databricks/*-dp.py; 
#   do
#       databricks workspace import --language PYTHON --format SOURCE --overwrite "${filename}" "/build/${filename##*/}"
#   done;
#   for filename in _Databricks/*-dp.scala; 
#   do
#       databricks workspace import --language SCALA --format SOURCE --overwrite "${filename}" "/build/${filename##*/}"
#   done;
#   echo

 